{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Model Testing: Chronos T5 for Home Price Prediction\n",
        "\n",
        "**Purpose**: Comprehensive testing and validation of the Chronos T5 model for home price forecasting\n",
        "\n",
        "## Overview\n",
        "\n",
        "This notebook provides comprehensive testing of the Chronos T5 Small model for home price prediction using Zillow ZHVI data. The testing framework follows modern 2025 data science best practices including:\n",
        "\n",
        "- **Model Performance Testing**: Accuracy metrics, backtesting, cross-validation\n",
        "- **Robustness Testing**: Edge cases, data quality issues, stress testing\n",
        "- **Production Readiness**: Error handling, monitoring, scalability assessment\n",
        "- **Statistical Validation**: Hypothesis testing, confidence intervals, bias analysis\n",
        "- **Interpretability**: Feature importance, prediction explainability\n",
        "\n",
        "## Table of Contents\n",
        "\n",
        "1. [Environment Setup & Configuration](#setup)\n",
        "2. [Data Loading & Validation](#data)\n",
        "3. [Model Initialization & Health Check](#model-init)\n",
        "4. [Basic Functionality Testing](#basic-tests)\n",
        "5. [Performance Benchmarking](#performance)\n",
        "6. [Robustness & Edge Case Testing](#robustness)\n",
        "7. [Statistical Validation](#statistics)\n",
        "8. [Production Readiness Assessment](#production)\n",
        "9. [Conclusions & Recommendations](#conclusions)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chronos package available\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-06-06 14:27:36,495 - utils - INFO - Logging configured with level: INFO\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All project modules imported successfully\n",
            "Environment setup complete\n",
            "Python version: 3.11.9 (tags/v3.11.9:de54cf5, Apr  2 2024, 10:12:12) [MSC v.1938 64 bit (AMD64)]\n",
            "NumPy version: 2.2.6\n",
            "Pandas version: 2.2.3\n",
            "PyTorch version: 2.7.0+cpu\n",
            "CUDA available: False\n",
            "Configuration loaded: Environment.DEVELOPMENT\n",
            "All model components ready for testing\n"
          ]
        }
      ],
      "source": [
        "# Environment Setup & Configuration\n",
        "import os\n",
        "import sys\n",
        "import warnings\n",
        "import logging\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Optional, Tuple, Any, Union\n",
        "import json\n",
        "import time\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# Data processing and analysis\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy import stats\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "# Machine learning and model evaluation\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "import torch\n",
        "\n",
        "# Check for chronos package (required for models)\n",
        "try:\n",
        "    from chronos import ChronosPipeline\n",
        "    chronos_available = True\n",
        "    print(\"Chronos package available\")\n",
        "except ImportError:\n",
        "    chronos_available = False\n",
        "    print(\"Chronos package not found. Install with: pip install chronos-forecasting\")\n",
        "\n",
        "# Project modules - add paths for imports\n",
        "sys.path.append('../src')\n",
        "sys.path.append('../config')\n",
        "\n",
        "# Import with error handling\n",
        "try:\n",
        "    from data_processor import ZillowDataProcessor\n",
        "    if chronos_available:\n",
        "        from model import ChronosT5Model  \n",
        "        from predictor import HomePricePredictor\n",
        "    else:\n",
        "        print(\"Skipping model imports due to missing chronos package\")\n",
        "        ChronosT5Model = None\n",
        "        HomePricePredictor = None\n",
        "    from utils import validate_zip_code, setup_logging\n",
        "    \n",
        "    # Try to import load_config\n",
        "    try:\n",
        "        from settings import load_config\n",
        "    except ImportError:\n",
        "        # Fallback if load_config not available\n",
        "        def load_config():\n",
        "            class MockConfig:\n",
        "                environment = \"development\"\n",
        "                data = type('obj', (object,), {'raw_data_file': '../data/raw/zhvi_zip.csv'})\n",
        "                model = type('obj', (object,), {'name': 'amazon/chronos-t5-small', 'device': 'auto'})\n",
        "                paths = type('obj', (object,), {'model_cache_dir': '../data/model_cache/'})\n",
        "            return MockConfig()\n",
        "    \n",
        "    # Try to import constants\n",
        "    try:\n",
        "        import constants\n",
        "    except ImportError:\n",
        "        # If constants module not available, define minimal constants\n",
        "        class Constants:\n",
        "            pass\n",
        "        constants = Constants()\n",
        "        \n",
        "    print(\"All project modules imported successfully\")\n",
        "    \n",
        "except ImportError as e:\n",
        "    print(f\"Error importing project modules: {e}\")\n",
        "    print(\"Please ensure you're running this notebook from the notebooks/ directory\")\n",
        "    print(\"and that all source files are in the ../src/ directory\")\n",
        "    raise\n",
        "\n",
        "# Configuration\n",
        "warnings.filterwarnings('ignore')\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', 100)\n",
        "plt.style.use('default')  # Updated from deprecated seaborn-v0_8\n",
        "sns.set_theme(style=\"whitegrid\", palette=\"husl\")  # Updated seaborn configuration\n",
        "\n",
        "# Setup logging for the notebook\n",
        "setup_logging()\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Load configuration\n",
        "config = load_config()\n",
        "\n",
        "print(\"Environment setup complete\")\n",
        "print(f\"Python version: {sys.version}\")\n",
        "print(f\"NumPy version: {np.__version__}\")\n",
        "print(f\"Pandas version: {pd.__version__}\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
        "print(f\"Configuration loaded: {config.environment}\")\n",
        "\n",
        "# Check if all required components are available\n",
        "if chronos_available and ChronosT5Model is not None:\n",
        "    print(\"All model components ready for testing\")\n",
        "else:\n",
        "    print(\"Some components unavailable - limited testing possible\")\n",
        "    print(\"Install missing packages to enable full testing\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Testing Configuration\n",
        "\n",
        "Define comprehensive testing parameters and helper functions for model evaluation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing configuration loaded\n",
            "Test ZIP codes: ['90210', '10001', '60601', '94102', '33101']\n",
            "Forecast horizons: [1, 3, 6, 12] months\n",
            "Performance thresholds: {'max_mae_percentage': 15.0, 'min_r2_score': 0.7, 'max_response_time_ms': 5000}\n",
            "Statistical testing config: {'significance_level': 0.05, 'bootstrap_samples': 1000, 'backtesting_periods': 12, 'min_training_periods': 36}\n"
          ]
        }
      ],
      "source": [
        "# Testing Configuration\n",
        "TEST_CONFIG = {\n",
        "    'test_zip_codes': ['90210', '10001', '60601', '94102', '33101'],  # Diverse set for testing\n",
        "    'forecast_horizons': [1, 3, 6, 12],  # months\n",
        "    'confidence_levels': [0.5, 0.8, 0.9],\n",
        "    'num_samples': 100,\n",
        "    'temperature': 1.0,\n",
        "    'random_seed': 42,\n",
        "    'performance_thresholds': {\n",
        "        'max_mae_percentage': 15.0,  # Maximum 15% MAE\n",
        "        'min_r2_score': 0.7,         # Minimum RÂ² of 0.7\n",
        "        'max_response_time_ms': 5000  # Maximum 5 seconds\n",
        "    }\n",
        "}\n",
        "\n",
        "# Statistical testing parameters\n",
        "STAT_CONFIG = {\n",
        "    'significance_level': 0.05,\n",
        "    'bootstrap_samples': 1000,\n",
        "    'backtesting_periods': 12,  # months\n",
        "    'min_training_periods': 36  # months\n",
        "}\n",
        "\n",
        "# Helper functions for testing\n",
        "def calculate_mape(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
        "    \"\"\"Calculate Mean Absolute Percentage Error.\"\"\"\n",
        "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
        "\n",
        "def calculate_smape(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
        "    \"\"\"Calculate Symmetric Mean Absolute Percentage Error.\"\"\"\n",
        "    return 100 * np.mean(2 * np.abs(y_pred - y_true) / (np.abs(y_true) + np.abs(y_pred)))\n",
        "\n",
        "def directional_accuracy(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
        "    \"\"\"Calculate directional accuracy (percentage of correct direction predictions).\"\"\"\n",
        "    true_diff = np.diff(y_true)\n",
        "    pred_diff = np.diff(y_pred)\n",
        "    return np.mean(np.sign(true_diff) == np.sign(pred_diff)) * 100\n",
        "\n",
        "def calculate_comprehensive_metrics(y_true: np.ndarray, y_pred: np.ndarray) -> Dict[str, float]:\n",
        "    \"\"\"Calculate comprehensive evaluation metrics.\"\"\"\n",
        "    return {\n",
        "        'mae': mean_absolute_error(y_true, y_pred),\n",
        "        'mse': mean_squared_error(y_true, y_pred),\n",
        "        'rmse': np.sqrt(mean_squared_error(y_true, y_pred)),\n",
        "        'r2': r2_score(y_true, y_pred),\n",
        "        'mape': calculate_mape(y_true, y_pred),\n",
        "        'smape': calculate_smape(y_true, y_pred),\n",
        "        'directional_accuracy': directional_accuracy(y_true, y_pred),\n",
        "        'mean_actual': np.mean(y_true),\n",
        "        'mean_predicted': np.mean(y_pred),\n",
        "        'std_actual': np.std(y_true),\n",
        "        'std_predicted': np.std(y_pred)\n",
        "    }\n",
        "\n",
        "def create_metrics_summary_table(metrics_dict: Dict[str, Dict[str, float]]) -> pd.DataFrame:\n",
        "    \"\"\"Create a summary table of metrics across different test cases.\"\"\"\n",
        "    return pd.DataFrame(metrics_dict).T.round(3)\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(TEST_CONFIG['random_seed'])\n",
        "torch.manual_seed(TEST_CONFIG['random_seed'])\n",
        "\n",
        "print(\"Testing configuration loaded\")\n",
        "print(f\"Test ZIP codes: {TEST_CONFIG['test_zip_codes']}\")\n",
        "print(f\"Forecast horizons: {TEST_CONFIG['forecast_horizons']} months\")\n",
        "print(f\"Performance thresholds: {TEST_CONFIG['performance_thresholds']}\")\n",
        "print(f\"Statistical testing config: {STAT_CONFIG}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Data Loading & Validation\n",
        "\n",
        "Load test data and validate data quality for comprehensive model testing.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error loading data: ZHVI data file not found: data/raw/zhvi_zip.csv\n"
          ]
        },
        {
          "ename": "FileNotFoundError",
          "evalue": "ZHVI data file not found: data/raw/zhvi_zip.csv",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m processor = ZillowDataProcessor(data_path)\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     \u001b[43mprocessor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mData loaded successfully\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      9\u001b[39m     \u001b[38;5;66;03m# Get data summary\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gillu\\Downloads\\USHMP\\notebooks\\../src\\data_processor.py:54\u001b[39m, in \u001b[36mZillowDataProcessor.load_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     52\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Load the ZHVI data from CSV file.\"\"\"\u001b[39;00m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.exists(\u001b[38;5;28mself\u001b[39m.data_path):\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mZHVI data file not found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.data_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     57\u001b[39m     logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLoading ZHVI data from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.data_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
            "\u001b[31mFileNotFoundError\u001b[39m: ZHVI data file not found: data/raw/zhvi_zip.csv"
          ]
        }
      ],
      "source": [
        "# Get the project root directory (parent of notebooks directory)\n",
        "project_root = Path.cwd().parent\n",
        "print(f\"Project root: {project_root}\")\n",
        "\n",
        "# Ensure we're working from the project root for path resolution\n",
        "os.chdir(project_root)\n",
        "print(f\"Changed working directory to: {os.getcwd()}\")\n",
        "\n",
        "# Now verify the data file exists\n",
        "data_path = project_root / \"data\" / \"raw\" / \"zhvi_zip.csv\"\n",
        "print(f\"Data file exists: {data_path.exists()}\")\n",
        "print(f\"Data file path: {data_path}\")\n",
        "\n",
        "# Initialize data processor\n",
        "data_path = config.data.raw_data_file\n",
        "processor = ZillowDataProcessor(data_path)\n",
        "\n",
        "try:\n",
        "    processor.load_data()\n",
        "    print(\"Data loaded successfully\")\n",
        "    \n",
        "    # Get data summary\n",
        "    summary = processor.get_data_summary()\n",
        "    print(\"Data summary:\")\n",
        "    for key, value in summary.items():\n",
        "        print(f\"  {key}: {value}\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"Error loading data: {e}\")\n",
        "    raise\n",
        "\n",
        "# Validate test ZIP codes\n",
        "print(\"\\nValidating test ZIP codes:\")\n",
        "test_data = {}\n",
        "for zip_code in TEST_CONFIG['test_zip_codes']:\n",
        "    try:\n",
        "        # Get time series data\n",
        "        ts_data = processor.get_zip_time_series(zip_code)\n",
        "        if ts_data is not None and len(ts_data) >= STAT_CONFIG['min_training_periods']:\n",
        "            test_data[zip_code] = ts_data\n",
        "            print(f\"  PASS {zip_code}: {len(ts_data)} data points, \"\n",
        "                  f\"${ts_data.iloc[-1]:,.0f} current value\")\n",
        "        else:\n",
        "            print(f\"  FAIL {zip_code}: Insufficient data\")\n",
        "    except Exception as e:\n",
        "        print(f\"  ERROR {zip_code}: {e}\")\n",
        "\n",
        "print(f\"\\n{len(test_data)} ZIP codes validated for testing\")\n",
        "\n",
        "# Data quality analysis\n",
        "print(\"\\nData Quality Analysis:\")\n",
        "quality_metrics = {}\n",
        "for zip_code, ts_data in test_data.items():\n",
        "    missing_pct = (len(ts_data) - ts_data.count()) / len(ts_data) * 100\n",
        "    volatility = ts_data.pct_change().std() * 100\n",
        "    trend = (ts_data.iloc[-1] - ts_data.iloc[0]) / ts_data.iloc[0] * 100\n",
        "    \n",
        "    quality_metrics[zip_code] = {\n",
        "        'data_points': len(ts_data),\n",
        "        'missing_pct': missing_pct,\n",
        "        'volatility_pct': volatility,\n",
        "        'total_return_pct': trend,\n",
        "        'current_value': ts_data.iloc[-1],\n",
        "        'date_range': f\"{ts_data.index[0].strftime('%Y-%m')} to {ts_data.index[-1].strftime('%Y-%m')}\"\n",
        "    }\n",
        "\n",
        "quality_df = pd.DataFrame(quality_metrics).T\n",
        "print(quality_df.round(2))\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Model Initialization & Health Check\n",
        "\n",
        "Initialize the Chronos T5 model and perform comprehensive health checks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-06-06 14:29:26,430 - model - INFO - Initializing Chronos T5 model on device: auto\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initializing Chronos T5 model...\n",
            "Running model health check...\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'test_data' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 151\u001b[39m\n\u001b[32m    148\u001b[39m health_checker = ModelHealthChecker(model)\n\u001b[32m    150\u001b[39m \u001b[38;5;66;03m# Get sample data for testing\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m151\u001b[39m sample_zip = \u001b[38;5;28mlist\u001b[39m(test_data.keys())[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtest_data\u001b[49m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    152\u001b[39m sample_data = test_data[sample_zip] \u001b[38;5;28;01mif\u001b[39;00m sample_zip \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sample_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    155\u001b[39m     \u001b[38;5;66;03m# Run all health checks\u001b[39;00m\n",
            "\u001b[31mNameError\u001b[39m: name 'test_data' is not defined"
          ]
        }
      ],
      "source": [
        "# Model Health Check Class\n",
        "class ModelHealthChecker:\n",
        "    def __init__(self, model: ChronosT5Model):\n",
        "        self.model = model\n",
        "        self.health_results = {}\n",
        "    \n",
        "    def check_model_loading(self) -> bool:\n",
        "        \"\"\"Test if model loads correctly.\"\"\"\n",
        "        try:\n",
        "            start_time = time.time()\n",
        "            self.model.load_model()\n",
        "            load_time = time.time() - start_time\n",
        "            \n",
        "            self.health_results['model_loading'] = {\n",
        "                'status': 'passed',\n",
        "                'load_time_seconds': load_time,\n",
        "                'model_info': self.model.get_model_info()\n",
        "            }\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            self.health_results['model_loading'] = {\n",
        "                'status': 'failed',\n",
        "                'error': str(e)\n",
        "            }\n",
        "            return False\n",
        "    \n",
        "    def check_basic_inference(self, sample_data: pd.Series) -> bool:\n",
        "        \"\"\"Test basic inference functionality.\"\"\"\n",
        "        try:\n",
        "            start_time = time.time()\n",
        "            result = self.model.predict_single_value(\n",
        "                time_series=sample_data.values[-50:],  # Use last 50 points\n",
        "                forecast_horizon=3\n",
        "            )\n",
        "            inference_time = time.time() - start_time\n",
        "            \n",
        "            self.health_results['basic_inference'] = {\n",
        "                'status': 'passed',\n",
        "                'inference_time_ms': inference_time * 1000,\n",
        "                'prediction_value': result.get('mean_forecast', 'N/A'),\n",
        "                'confidence_interval': result.get('confidence_interval', 'N/A')\n",
        "            }\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            self.health_results['basic_inference'] = {\n",
        "                'status': 'failed',\n",
        "                'error': str(e)\n",
        "            }\n",
        "            return False\n",
        "    \n",
        "    def check_input_validation(self) -> bool:\n",
        "        \"\"\"Test input validation and error handling.\"\"\"\n",
        "        test_cases = [\n",
        "            ('empty_input', []),\n",
        "            ('single_value', [100000]),\n",
        "            ('insufficient_data', [100000, 105000]),\n",
        "            ('nan_values', [100000, np.nan, 105000, 110000]),\n",
        "            ('negative_values', [-100000, 105000, 110000]),\n",
        "            ('extreme_values', [1e10, 1e11, 1e12])\n",
        "        ]\n",
        "        \n",
        "        validation_results = {}\n",
        "        for test_name, test_data in test_cases:\n",
        "            try:\n",
        "                result = self.model.predict_single_value(\n",
        "                    time_series=test_data,\n",
        "                    forecast_horizon=1\n",
        "                )\n",
        "                validation_results[test_name] = 'unexpected_success'\n",
        "            except Exception as e:\n",
        "                validation_results[test_name] = f'expected_error: {type(e).__name__}'\n",
        "        \n",
        "        self.health_results['input_validation'] = {\n",
        "            'status': 'completed',\n",
        "            'test_results': validation_results\n",
        "        }\n",
        "        return True\n",
        "    \n",
        "    def check_memory_usage(self, sample_data: pd.Series) -> bool:\n",
        "        \"\"\"Test memory usage during inference.\"\"\"\n",
        "        try:\n",
        "            import gc\n",
        "            import torch\n",
        "            \n",
        "            # Clear cache and collect garbage\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "            \n",
        "            # Get baseline GPU memory if available\n",
        "            baseline_gpu_memory = 0\n",
        "            if torch.cuda.is_available():\n",
        "                baseline_gpu_memory = torch.cuda.memory_allocated() / 1024 / 1024  # MB\n",
        "            \n",
        "            # Run multiple predictions\n",
        "            for i in range(5):\n",
        "                self.model.predict_single_value(\n",
        "                    time_series=sample_data.values[-30:],\n",
        "                    forecast_horizon=6\n",
        "                )\n",
        "            \n",
        "            # Check memory after predictions\n",
        "            final_gpu_memory = 0\n",
        "            if torch.cuda.is_available():\n",
        "                final_gpu_memory = torch.cuda.memory_allocated() / 1024 / 1024  # MB\n",
        "            \n",
        "            memory_increase = final_gpu_memory - baseline_gpu_memory\n",
        "            \n",
        "            self.health_results['memory_usage'] = {\n",
        "                'status': 'passed',\n",
        "                'baseline_gpu_memory_mb': baseline_gpu_memory,\n",
        "                'final_gpu_memory_mb': final_gpu_memory,\n",
        "                'memory_increase_mb': memory_increase,\n",
        "                'cuda_available': torch.cuda.is_available()\n",
        "            }\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            self.health_results['memory_usage'] = {\n",
        "                'status': 'failed',\n",
        "                'error': str(e)\n",
        "            }\n",
        "            return False\n",
        "    \n",
        "    def generate_health_report(self) -> Dict:\n",
        "        \"\"\"Generate comprehensive health report.\"\"\"\n",
        "        passed_tests = sum(1 for result in self.health_results.values() \n",
        "                          if result.get('status') == 'passed')\n",
        "        total_tests = len(self.health_results)\n",
        "        \n",
        "        return {\n",
        "            'overall_health': 'healthy' if passed_tests == total_tests else 'issues_detected',\n",
        "            'tests_passed': passed_tests,\n",
        "            'total_tests': total_tests,\n",
        "            'detailed_results': self.health_results,\n",
        "            'timestamp': datetime.now().isoformat()\n",
        "        }\n",
        "\n",
        "# Initialize model and run health checks\n",
        "print(\"Initializing Chronos T5 model...\")\n",
        "model = ChronosT5Model(\n",
        "    model_name=config.model.name,\n",
        "    cache_dir=config.paths.model_cache_dir,\n",
        "    device=config.model.device\n",
        ")\n",
        "\n",
        "# Run comprehensive health check\n",
        "print(\"Running model health check...\")\n",
        "health_checker = ModelHealthChecker(model)\n",
        "\n",
        "# Get sample data for testing\n",
        "sample_zip = list(test_data.keys())[0] if test_data else None\n",
        "sample_data = test_data[sample_zip] if sample_zip else None\n",
        "\n",
        "if sample_data is not None:\n",
        "    # Run all health checks\n",
        "    health_checker.check_model_loading()\n",
        "    health_checker.check_basic_inference(sample_data)\n",
        "    health_checker.check_input_validation()\n",
        "    health_checker.check_memory_usage(sample_data)\n",
        "    \n",
        "    # Generate health report\n",
        "    health_report = health_checker.generate_health_report()\n",
        "    \n",
        "    print(f\"\\nModel Health Check Results:\")\n",
        "    print(f\"Overall Status: {health_report['overall_health'].upper()}\")\n",
        "    print(f\"Tests Passed: {health_report['tests_passed']}/{health_report['total_tests']}\")\n",
        "    \n",
        "    for test_name, results in health_report['detailed_results'].items():\n",
        "        status_indicator = \"PASS\" if results['status'] == 'passed' else \"FAIL\" if results['status'] == 'failed' else \"INFO\"\n",
        "        print(f\"{status_indicator} {test_name.replace('_', ' ').title()}: {results['status'].upper()}\")\n",
        "        \n",
        "        if 'load_time_seconds' in results:\n",
        "            print(f\"    Load time: {results['load_time_seconds']:.2f}s\")\n",
        "        if 'inference_time_ms' in results:\n",
        "            print(f\"    Inference time: {results['inference_time_ms']:.1f}ms\")\n",
        "        if 'memory_increase_mb' in results:\n",
        "            print(f\"    Memory usage: +{results['memory_increase_mb']:.1f}MB\")\n",
        "        if results['status'] == 'failed':\n",
        "            print(f\"    Error: {results.get('error', 'Unknown error')}\")\n",
        "\n",
        "else:\n",
        "    print(\"ERROR: No sample data available for health check\")\n",
        "    health_report = {'overall_health': 'no_data_available'}\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Basic Functionality Testing\n",
        "\n",
        "Test core model functionality across different scenarios and forecast horizons.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Basic Functionality Tests\n",
        "class BasicFunctionalityTester:\n",
        "    def __init__(self, model: ChronosT5Model, test_data: Dict[str, pd.Series]):\n",
        "        self.model = model\n",
        "        self.test_data = test_data\n",
        "        self.test_results = {}\n",
        "    \n",
        "    def test_single_predictions(self) -> Dict[str, Any]:\n",
        "        \"\"\"Test single prediction functionality.\"\"\"\n",
        "        print(\"Testing single predictions...\")\n",
        "        results = {}\n",
        "        \n",
        "        for zip_code, ts_data in self.test_data.items():\n",
        "            zip_results = {}\n",
        "            \n",
        "            for horizon in TEST_CONFIG['forecast_horizons']:\n",
        "                try:\n",
        "                    start_time = time.time()\n",
        "                    prediction = self.model.predict_single_value(\n",
        "                        time_series=ts_data.values,\n",
        "                        forecast_horizon=horizon\n",
        "                    )\n",
        "                    response_time = (time.time() - start_time) * 1000\n",
        "                    \n",
        "                    zip_results[f'{horizon}m'] = {\n",
        "                        'predicted_value': prediction.get('mean_forecast'),\n",
        "                        'confidence_interval': prediction.get('confidence_interval'),\n",
        "                        'response_time_ms': response_time,\n",
        "                        'status': 'success'\n",
        "                    }\n",
        "                    \n",
        "                except Exception as e:\n",
        "                    zip_results[f'{horizon}m'] = {\n",
        "                        'status': 'failed',\n",
        "                        'error': str(e)\n",
        "                    }\n",
        "            \n",
        "            results[zip_code] = zip_results\n",
        "            print(f\"  {zip_code}: Completed\")\n",
        "        \n",
        "        self.test_results['single_predictions'] = results\n",
        "        return results\n",
        "    \n",
        "    def test_batch_predictions(self) -> Dict[str, Any]:\n",
        "        \"\"\"Test batch prediction functionality.\"\"\"\n",
        "        print(\"Testing batch predictions...\")\n",
        "        \n",
        "        try:\n",
        "            time_series_list = list(self.test_data.values())\n",
        "            forecast_horizons = TEST_CONFIG['forecast_horizons']\n",
        "            \n",
        "            start_time = time.time()\n",
        "            batch_results = self.model.batch_predict(\n",
        "                time_series_list=time_series_list,\n",
        "                forecast_horizons=forecast_horizons,\n",
        "                num_samples=TEST_CONFIG['num_samples']\n",
        "            )\n",
        "            total_time = time.time() - start_time\n",
        "            \n",
        "            self.test_results['batch_predictions'] = {\n",
        "                'status': 'success',\n",
        "                'total_time_seconds': total_time,\n",
        "                'predictions_count': len(batch_results),\n",
        "                'avg_time_per_prediction': total_time / len(batch_results) if batch_results else 0,\n",
        "                'results': batch_results\n",
        "            }\n",
        "            \n",
        "            print(f\"  Batch prediction completed: {len(batch_results)} predictions in {total_time:.2f}s\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            self.test_results['batch_predictions'] = {\n",
        "                'status': 'failed',\n",
        "                'error': str(e)\n",
        "            }\n",
        "            print(f\"  Batch prediction failed: {e}\")\n",
        "        \n",
        "        return self.test_results['batch_predictions']\n",
        "    \n",
        "    def test_confidence_intervals(self) -> Dict[str, Any]:\n",
        "        \"\"\"Test confidence interval generation.\"\"\"\n",
        "        print(\"Testing confidence intervals...\")\n",
        "        results = {}\n",
        "        \n",
        "        # Use first ZIP code for detailed CI testing\n",
        "        zip_code = list(self.test_data.keys())[0]\n",
        "        ts_data = self.test_data[zip_code]\n",
        "        \n",
        "        for confidence_level in TEST_CONFIG['confidence_levels']:\n",
        "            try:\n",
        "                prediction = self.model.predict(\n",
        "                    time_series=ts_data.values,\n",
        "                    forecast_horizon=6,\n",
        "                    num_samples=TEST_CONFIG['num_samples']\n",
        "                )\n",
        "                \n",
        "                # Calculate custom confidence intervals\n",
        "                samples = np.array(prediction['samples'])\n",
        "                lower_percentile = (1 - confidence_level) / 2 * 100\n",
        "                upper_percentile = (1 + confidence_level) / 2 * 100\n",
        "                \n",
        "                ci_lower = np.percentile(samples, lower_percentile, axis=0)\n",
        "                ci_upper = np.percentile(samples, upper_percentile, axis=0)\n",
        "                mean_pred = np.mean(samples, axis=0)\n",
        "                \n",
        "                # Calculate CI width as percentage of prediction\n",
        "                ci_width_pct = ((ci_upper - ci_lower) / mean_pred * 100)\n",
        "                \n",
        "                results[f'ci_{confidence_level}'] = {\n",
        "                    'lower_bound': ci_lower.tolist(),\n",
        "                    'upper_bound': ci_upper.tolist(),\n",
        "                    'mean_prediction': mean_pred.tolist(),\n",
        "                    'avg_width_percentage': np.mean(ci_width_pct),\n",
        "                    'status': 'success'\n",
        "                }\n",
        "                \n",
        "            except Exception as e:\n",
        "                results[f'ci_{confidence_level}'] = {\n",
        "                    'status': 'failed',\n",
        "                    'error': str(e)\n",
        "                }\n",
        "        \n",
        "        self.test_results['confidence_intervals'] = results\n",
        "        print(\"  Confidence interval testing completed\")\n",
        "        return results\n",
        "    \n",
        "    def test_response_times(self) -> Dict[str, Any]:\n",
        "        \"\"\"Test response time performance.\"\"\"\n",
        "        print(\"Testing response times...\")\n",
        "        \n",
        "        # Test different input sizes\n",
        "        zip_code = list(self.test_data.keys())[0]\n",
        "        full_data = self.test_data[zip_code].values\n",
        "        \n",
        "        input_sizes = [12, 24, 50, 100, len(full_data)]\n",
        "        response_times = {}\n",
        "        \n",
        "        for size in input_sizes:\n",
        "            if size <= len(full_data):\n",
        "                times = []\n",
        "                for i in range(3):  # Run 3 times for average\n",
        "                    start_time = time.time()\n",
        "                    try:\n",
        "                        self.model.predict_single_value(\n",
        "                            time_series=full_data[-size:],\n",
        "                            forecast_horizon=3\n",
        "                        )\n",
        "                        times.append((time.time() - start_time) * 1000)\n",
        "                    except Exception as e:\n",
        "                        times.append(None)\n",
        "                \n",
        "                valid_times = [t for t in times if t is not None]\n",
        "                response_times[f'input_size_{size}'] = {\n",
        "                    'avg_response_time_ms': np.mean(valid_times) if valid_times else None,\n",
        "                    'min_response_time_ms': np.min(valid_times) if valid_times else None,\n",
        "                    'max_response_time_ms': np.max(valid_times) if valid_times else None,\n",
        "                    'success_rate': len(valid_times) / len(times) * 100\n",
        "                }\n",
        "        \n",
        "        self.test_results['response_times'] = response_times\n",
        "        print(\"  Response time testing completed\")\n",
        "        return response_times\n",
        "    \n",
        "    def generate_functionality_report(self) -> Dict[str, Any]:\n",
        "        \"\"\"Generate comprehensive functionality test report.\"\"\"\n",
        "        total_tests = 0\n",
        "        passed_tests = 0\n",
        "        \n",
        "        for test_category, results in self.test_results.items():\n",
        "            if isinstance(results, dict):\n",
        "                if test_category == 'single_predictions':\n",
        "                    for zip_results in results.values():\n",
        "                        for horizon_result in zip_results.values():\n",
        "                            total_tests += 1\n",
        "                            if horizon_result.get('status') == 'success':\n",
        "                                passed_tests += 1\n",
        "                elif results.get('status') == 'success':\n",
        "                    passed_tests += 1\n",
        "                    total_tests += 1\n",
        "                elif results.get('status') == 'failed':\n",
        "                    total_tests += 1\n",
        "        \n",
        "        return {\n",
        "            'overall_status': 'passed' if passed_tests == total_tests else 'partial',\n",
        "            'tests_passed': passed_tests,\n",
        "            'total_tests': total_tests,\n",
        "            'success_rate': (passed_tests / total_tests * 100) if total_tests > 0 else 0,\n",
        "            'detailed_results': self.test_results,\n",
        "            'timestamp': datetime.now().isoformat()\n",
        "        }\n",
        "\n",
        "# Run basic functionality tests\n",
        "if health_report.get('overall_health') == 'healthy':\n",
        "    print(\"Running basic functionality tests...\\n\")\n",
        "    \n",
        "    functionality_tester = BasicFunctionalityTester(model, test_data)\n",
        "    \n",
        "    # Run all tests\n",
        "    functionality_tester.test_single_predictions()\n",
        "    functionality_tester.test_batch_predictions()\n",
        "    functionality_tester.test_confidence_intervals()\n",
        "    functionality_tester.test_response_times()\n",
        "    \n",
        "    # Generate report\n",
        "    functionality_report = functionality_tester.generate_functionality_report()\n",
        "    \n",
        "    print(f\"\\nBasic Functionality Test Results:\")\n",
        "    print(f\"Overall Status: {functionality_report['overall_status'].upper()}\")\n",
        "    print(f\"Success Rate: {functionality_report['success_rate']:.1f}% ({functionality_report['tests_passed']}/{functionality_report['total_tests']})\")\n",
        "    \n",
        "    # Performance summary\n",
        "    response_times = functionality_report['detailed_results'].get('response_times', {})\n",
        "    if response_times:\n",
        "        avg_times = [result['avg_response_time_ms'] for result in response_times.values() \n",
        "                    if result.get('avg_response_time_ms') is not None]\n",
        "        if avg_times:\n",
        "            print(f\"Average Response Time: {np.mean(avg_times):.1f}ms\")\n",
        "            print(f\"Performance Threshold: {'PASSED' if np.mean(avg_times) < TEST_CONFIG['performance_thresholds']['max_response_time_ms'] else 'FAILED'}\")\n",
        "    \n",
        "else:\n",
        "    print(\"WARNING: Skipping basic functionality tests due to model health issues\")\n",
        "    functionality_report = {'overall_status': 'skipped'}\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Performance Benchmarking & Backtesting\n",
        "\n",
        "Evaluate model performance using statistical metrics and time series backtesting methodology.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Performance Benchmarking Class\n",
        "class PerformanceBenchmarker:\n",
        "    def __init__(self, model: ChronosT5Model, test_data: Dict[str, pd.Series]):\n",
        "        self.model = model\n",
        "        self.test_data = test_data\n",
        "        self.benchmark_results = {}\n",
        "    \n",
        "    def time_series_split_backtest(self, zip_code: str, forecast_horizon: int, n_splits: int = 5) -> Dict[str, Any]:\n",
        "        \"\"\"Perform time series cross-validation backtesting.\"\"\"\n",
        "        ts_data = self.test_data[zip_code]\n",
        "        \n",
        "        # Use TimeSeriesSplit for proper time series validation\n",
        "        tscv = TimeSeriesSplit(n_splits=n_splits)\n",
        "        \n",
        "        fold_results = []\n",
        "        \n",
        "        for fold, (train_idx, test_idx) in enumerate(tscv.split(ts_data)):\n",
        "            try:\n",
        "                # Ensure we have enough data for training\n",
        "                if len(train_idx) < STAT_CONFIG['min_training_periods']:\n",
        "                    continue\n",
        "                \n",
        "                # Split data\n",
        "                train_data = ts_data.iloc[train_idx]\n",
        "                test_data = ts_data.iloc[test_idx[:forecast_horizon]]  # Only test next 'forecast_horizon' points\n",
        "                \n",
        "                if len(test_data) < forecast_horizon:\n",
        "                    continue\n",
        "                \n",
        "                # Make prediction\n",
        "                prediction = self.model.predict_single_value(\n",
        "                    time_series=train_data.values,\n",
        "                    forecast_horizon=len(test_data)\n",
        "                )\n",
        "                \n",
        "                predicted_values = prediction['mean_forecast']\n",
        "                actual_values = test_data.values\n",
        "                \n",
        "                # Calculate metrics\n",
        "                fold_metrics = calculate_comprehensive_metrics(actual_values, predicted_values)\n",
        "                fold_metrics['fold'] = fold\n",
        "                fold_metrics['train_size'] = len(train_data)\n",
        "                fold_metrics['test_size'] = len(test_data)\n",
        "                \n",
        "                fold_results.append(fold_metrics)\n",
        "                \n",
        "            except Exception as e:\n",
        "                logger.warning(f\"Fold {fold} failed for {zip_code}: {e}\")\n",
        "                continue\n",
        "        \n",
        "        if fold_results:\n",
        "            # Aggregate results across folds\n",
        "            metrics_df = pd.DataFrame(fold_results)\n",
        "            aggregate_results = {\n",
        "                'mean_metrics': metrics_df.select_dtypes(include=[np.number]).mean().to_dict(),\n",
        "                'std_metrics': metrics_df.select_dtypes(include=[np.number]).std().to_dict(),\n",
        "                'fold_results': fold_results,\n",
        "                'n_successful_folds': len(fold_results)\n",
        "            }\n",
        "        else:\n",
        "            aggregate_results = {'error': 'All folds failed', 'n_successful_folds': 0}\n",
        "        \n",
        "        return aggregate_results\n",
        "    \n",
        "    def comprehensive_accuracy_test(self) -> Dict[str, Any]:\n",
        "        \"\"\"Run comprehensive accuracy tests across all ZIP codes and horizons.\"\"\"\n",
        "        print(\"Running comprehensive accuracy tests...\")\n",
        "        \n",
        "        accuracy_results = {}\n",
        "        \n",
        "        for zip_code in self.test_data.keys():\n",
        "            print(f\"  Testing {zip_code}...\")\n",
        "            zip_results = {}\n",
        "            \n",
        "            for horizon in TEST_CONFIG['forecast_horizons']:\n",
        "                try:\n",
        "                    backtest_result = self.time_series_split_backtest(zip_code, horizon)\n",
        "                    zip_results[f'{horizon}m'] = backtest_result\n",
        "                except Exception as e:\n",
        "                    zip_results[f'{horizon}m'] = {'error': str(e)}\n",
        "            \n",
        "            accuracy_results[zip_code] = zip_results\n",
        "        \n",
        "        self.benchmark_results['accuracy_tests'] = accuracy_results\n",
        "        return accuracy_results\n",
        "    \n",
        "    def naive_forecast_baseline(self) -> Dict[str, Any]:\n",
        "        \"\"\"Compare against naive forecasting baselines.\"\"\"\n",
        "        print(\"Computing baseline comparisons...\")\n",
        "        \n",
        "        baseline_results = {}\n",
        "        \n",
        "        for zip_code, ts_data in self.test_data.items():\n",
        "            zip_baselines = {}\n",
        "            \n",
        "            # Split data for testing (use last 12 months as test)\n",
        "            test_size = min(12, len(ts_data) // 4)\n",
        "            train_data = ts_data.iloc[:-test_size]\n",
        "            test_data = ts_data.iloc[-test_size:]\n",
        "            \n",
        "            for horizon in [1, 3, 6]:  # Test shorter horizons for baseline comparison\n",
        "                if horizon <= len(test_data):\n",
        "                    actual_values = test_data.iloc[:horizon].values\n",
        "                    \n",
        "                    # Naive baselines\n",
        "                    last_value_forecast = np.full(horizon, train_data.iloc[-1])\n",
        "                    seasonal_naive = train_data.iloc[-12:].values  # Last 12 months\n",
        "                    if len(seasonal_naive) >= horizon:\n",
        "                        seasonal_naive_forecast = seasonal_naive[:horizon]\n",
        "                    else:\n",
        "                        seasonal_naive_forecast = np.full(horizon, train_data.iloc[-1])\n",
        "                    \n",
        "                    # Model prediction\n",
        "                    try:\n",
        "                        model_prediction = self.model.predict_single_value(\n",
        "                            time_series=train_data.values,\n",
        "                            forecast_horizon=horizon\n",
        "                        )\n",
        "                        model_forecast = model_prediction['mean_forecast']\n",
        "                        \n",
        "                        # Calculate metrics for each method\n",
        "                        zip_baselines[f'{horizon}m'] = {\n",
        "                            'model_metrics': calculate_comprehensive_metrics(actual_values, model_forecast),\n",
        "                            'naive_metrics': calculate_comprehensive_metrics(actual_values, last_value_forecast),\n",
        "                            'seasonal_naive_metrics': calculate_comprehensive_metrics(actual_values, seasonal_naive_forecast),\n",
        "                            'model_improvement_mae': (calculate_comprehensive_metrics(actual_values, last_value_forecast)['mae'] - \n",
        "                                                    calculate_comprehensive_metrics(actual_values, model_forecast)['mae']),\n",
        "                            'status': 'success'\n",
        "                        }\n",
        "                    except Exception as e:\n",
        "                        zip_baselines[f'{horizon}m'] = {'error': str(e), 'status': 'failed'}\n",
        "            \n",
        "            baseline_results[zip_code] = zip_baselines\n",
        "        \n",
        "        self.benchmark_results['baseline_comparison'] = baseline_results\n",
        "        return baseline_results\n",
        "    \n",
        "    def performance_vs_data_length(self) -> Dict[str, Any]:\n",
        "        \"\"\"Test how performance varies with input data length.\"\"\"\n",
        "        print(\"Testing performance vs data length...\")\n",
        "        \n",
        "        # Use the ZIP code with most data\n",
        "        zip_code = max(self.test_data.keys(), key=lambda k: len(self.test_data[k]))\n",
        "        full_data = self.test_data[zip_code]\n",
        "        \n",
        "        # Test different input lengths\n",
        "        data_lengths = [24, 36, 60, 120, len(full_data)]\n",
        "        length_results = {}\n",
        "        \n",
        "        # Use consistent test period (last 6 months)\n",
        "        test_size = 6\n",
        "        test_data = full_data.iloc[-test_size:]\n",
        "        \n",
        "        for length in data_lengths:\n",
        "            if length <= len(full_data) - test_size:\n",
        "                try:\n",
        "                    train_data = full_data.iloc[-(length + test_size):-test_size]\n",
        "                    \n",
        "                    prediction = self.model.predict_single_value(\n",
        "                        time_series=train_data.values,\n",
        "                        forecast_horizon=test_size\n",
        "                    )\n",
        "                    \n",
        "                    predicted_values = prediction['mean_forecast']\n",
        "                    actual_values = test_data.values\n",
        "                    \n",
        "                    metrics = calculate_comprehensive_metrics(actual_values, predicted_values)\n",
        "                    metrics['input_length'] = length\n",
        "                    metrics['status'] = 'success'\n",
        "                    \n",
        "                    length_results[f'length_{length}'] = metrics\n",
        "                    \n",
        "                except Exception as e:\n",
        "                    length_results[f'length_{length}'] = {'error': str(e), 'status': 'failed'}\n",
        "        \n",
        "        self.benchmark_results['data_length_analysis'] = {\n",
        "            'zip_code_tested': zip_code,\n",
        "            'results': length_results\n",
        "        }\n",
        "        return length_results\n",
        "    \n",
        "    def generate_performance_summary(self) -> Dict[str, Any]:\n",
        "        \"\"\"Generate comprehensive performance summary.\"\"\"\n",
        "        summary = {\n",
        "            'timestamp': datetime.now().isoformat(),\n",
        "            'total_zip_codes_tested': len(self.test_data),\n",
        "            'test_configurations': TEST_CONFIG,\n",
        "            'statistical_config': STAT_CONFIG\n",
        "        }\n",
        "        \n",
        "        # Aggregate accuracy results\n",
        "        if 'accuracy_tests' in self.benchmark_results:\n",
        "            all_metrics = []\n",
        "            for zip_results in self.benchmark_results['accuracy_tests'].values():\n",
        "                for horizon_results in zip_results.values():\n",
        "                    if 'mean_metrics' in horizon_results:\n",
        "                        all_metrics.append(horizon_results['mean_metrics'])\n",
        "            \n",
        "            if all_metrics:\n",
        "                metrics_df = pd.DataFrame(all_metrics)\n",
        "                summary['overall_performance'] = {\n",
        "                    'mean_mae': metrics_df['mae'].mean(),\n",
        "                    'mean_mape': metrics_df['mape'].mean(),\n",
        "                    'mean_r2': metrics_df['r2'].mean(),\n",
        "                    'mean_directional_accuracy': metrics_df['directional_accuracy'].mean()\n",
        "                }\n",
        "        \n",
        "        # Baseline comparison summary\n",
        "        if 'baseline_comparison' in self.benchmark_results:\n",
        "            improvements = []\n",
        "            for zip_results in self.benchmark_results['baseline_comparison'].values():\n",
        "                for horizon_results in zip_results.values():\n",
        "                    if horizon_results.get('status') == 'success':\n",
        "                        improvements.append(horizon_results['model_improvement_mae'])\n",
        "            \n",
        "            if improvements:\n",
        "                summary['baseline_performance'] = {\n",
        "                    'mean_mae_improvement': np.mean(improvements),\n",
        "                    'improvement_std': np.std(improvements),\n",
        "                    'percent_improved': (np.array(improvements) > 0).mean() * 100\n",
        "                }\n",
        "        \n",
        "        summary['detailed_results'] = self.benchmark_results\n",
        "        return summary\n",
        "\n",
        "# Run performance benchmarking\n",
        "if functionality_report.get('overall_status') in ['passed', 'partial']:\n",
        "    print(\"Running performance benchmarking...\\n\")\n",
        "    \n",
        "    benchmarker = PerformanceBenchmarker(model, test_data)\n",
        "    \n",
        "    # Run all benchmark tests\n",
        "    benchmarker.comprehensive_accuracy_test()\n",
        "    benchmarker.naive_forecast_baseline()\n",
        "    benchmarker.performance_vs_data_length()\n",
        "    \n",
        "    # Generate performance summary\n",
        "    performance_summary = benchmarker.generate_performance_summary()\n",
        "    \n",
        "    print(f\"\\nPerformance Benchmarking Results:\")\n",
        "    \n",
        "    if 'overall_performance' in performance_summary:\n",
        "        perf = performance_summary['overall_performance']\n",
        "        print(f\"Overall Model Performance:\")\n",
        "        print(f\"  Mean MAE: {perf['mean_mae']:,.0f}\")\n",
        "        print(f\"  Mean MAPE: {perf['mean_mape']:.1f}%\")\n",
        "        print(f\"  Mean RÂ²: {perf['mean_r2']:.3f}\")\n",
        "        print(f\"  Directional Accuracy: {perf['mean_directional_accuracy']:.1f}%\")\n",
        "        \n",
        "        # Check against thresholds\n",
        "        mae_threshold_check = perf['mean_mape'] <= TEST_CONFIG['performance_thresholds']['max_mae_percentage']\n",
        "        r2_threshold_check = perf['mean_r2'] >= TEST_CONFIG['performance_thresholds']['min_r2_score']\n",
        "        \n",
        "        print(f\"Performance Thresholds:\")\n",
        "        print(f\"  MAE Threshold: {'PASSED' if mae_threshold_check else 'FAILED'}\")\n",
        "        print(f\"  RÂ² Threshold: {'PASSED' if r2_threshold_check else 'FAILED'}\")\n",
        "    \n",
        "    if 'baseline_performance' in performance_summary:\n",
        "        baseline = performance_summary['baseline_performance']\n",
        "        print(f\"\\nBaseline Comparison:\")\n",
        "        print(f\"  Mean MAE Improvement: {baseline['mean_mae_improvement']:,.0f}\")\n",
        "        print(f\"  Improvement Rate: {baseline['percent_improved']:.1f}% of cases\")\n",
        "    \n",
        "else:\n",
        "    print(\"WARNING: Skipping performance benchmarking due to functionality test issues\")\n",
        "    performance_summary = {'status': 'skipped'}\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Robustness & Edge Case Testing\n",
        "\n",
        "Test model robustness against various edge cases and data quality issues.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Robustness Testing Class\n",
        "class RobustnessTester:\n",
        "    def __init__(self, model: ChronosT5Model, test_data: Dict[str, pd.Series]):\n",
        "        self.model = model\n",
        "        self.test_data = test_data\n",
        "        self.robustness_results = {}\n",
        "    \n",
        "    def test_data_corruption(self) -> Dict[str, Any]:\n",
        "        \"\"\"Test model behavior with corrupted data.\"\"\"\n",
        "        print(\"Testing data corruption scenarios...\")\n",
        "        \n",
        "        # Use first ZIP code for corruption testing\n",
        "        zip_code = list(self.test_data.keys())[0]\n",
        "        original_data = self.test_data[zip_code].values.copy()\n",
        "        \n",
        "        corruption_tests = {\n",
        "            'missing_values_10pct': self._inject_missing_values(original_data, 0.1),\n",
        "            'missing_values_25pct': self._inject_missing_values(original_data, 0.25),\n",
        "            'outliers_extreme': self._inject_outliers(original_data, 0.05, 10),\n",
        "            'outliers_moderate': self._inject_outliers(original_data, 0.1, 3),\n",
        "            'noise_gaussian': self._add_gaussian_noise(original_data, 0.1),\n",
        "            'trend_break': self._inject_trend_break(original_data),\n",
        "        }\n",
        "        \n",
        "        corruption_results = {}\n",
        "        \n",
        "        for test_name, corrupted_data in corruption_tests.items():\n",
        "            try:\n",
        "                prediction = self.model.predict_single_value(\n",
        "                    time_series=corrupted_data,\n",
        "                    forecast_horizon=3\n",
        "                )\n",
        "                \n",
        "                # Compare with original prediction\n",
        "                original_prediction = self.model.predict_single_value(\n",
        "                    time_series=original_data,\n",
        "                    forecast_horizon=3\n",
        "                )\n",
        "                \n",
        "                prediction_diff = abs(prediction['mean_forecast'] - original_prediction['mean_forecast'])\n",
        "                relative_diff = prediction_diff / original_prediction['mean_forecast'] * 100\n",
        "                \n",
        "                corruption_results[test_name] = {\n",
        "                    'status': 'success',\n",
        "                    'prediction_value': prediction['mean_forecast'],\n",
        "                    'original_prediction': original_prediction['mean_forecast'],\n",
        "                    'absolute_difference': prediction_diff,\n",
        "                    'relative_difference_pct': relative_diff,\n",
        "                    'stability_score': 100 - min(relative_diff, 100)  # Higher is more stable\n",
        "                }\n",
        "                \n",
        "            except Exception as e:\n",
        "                corruption_results[test_name] = {\n",
        "                    'status': 'failed',\n",
        "                    'error': str(e)\n",
        "                }\n",
        "        \n",
        "        self.robustness_results['data_corruption'] = corruption_results\n",
        "        return corruption_results\n",
        "    \n",
        "    def test_input_variations(self) -> Dict[str, Any]:\n",
        "        \"\"\"Test model with various input variations.\"\"\"\n",
        "        print(\"Testing input variations...\")\n",
        "        \n",
        "        zip_code = list(self.test_data.keys())[0]\n",
        "        base_data = self.test_data[zip_code].values\n",
        "        \n",
        "        variation_tests = {\n",
        "            'minimum_length': base_data[-12:],  # Minimum required length\n",
        "            'very_short': base_data[-15:],      # Slightly above minimum\n",
        "            'medium_length': base_data[-36:],   # 3 years\n",
        "            'long_history': base_data[-120:],   # 10 years\n",
        "            'full_history': base_data,          # Full available data\n",
        "        }\n",
        "        \n",
        "        variation_results = {}\n",
        "        \n",
        "        for test_name, test_data_variant in variation_tests.items():\n",
        "            try:\n",
        "                start_time = time.time()\n",
        "                prediction = self.model.predict_single_value(\n",
        "                    time_series=test_data_variant,\n",
        "                    forecast_horizon=6\n",
        "                )\n",
        "                response_time = (time.time() - start_time) * 1000\n",
        "                \n",
        "                variation_results[test_name] = {\n",
        "                    'status': 'success',\n",
        "                    'input_length': len(test_data_variant),\n",
        "                    'prediction_value': prediction['mean_forecast'],\n",
        "                    'confidence_interval': prediction.get('confidence_interval'),\n",
        "                    'response_time_ms': response_time\n",
        "                }\n",
        "                \n",
        "            except Exception as e:\n",
        "                variation_results[test_name] = {\n",
        "                    'status': 'failed',\n",
        "                    'input_length': len(test_data_variant),\n",
        "                    'error': str(e)\n",
        "                }\n",
        "        \n",
        "        self.robustness_results['input_variations'] = variation_results\n",
        "        return variation_results\n",
        "    \n",
        "    def test_extreme_scenarios(self) -> Dict[str, Any]:\n",
        "        \"\"\"Test model with extreme market scenarios.\"\"\"\n",
        "        print(\"Testing extreme scenarios...\")\n",
        "        \n",
        "        zip_code = list(self.test_data.keys())[0]\n",
        "        base_data = self.test_data[zip_code].values\n",
        "        \n",
        "        # Create extreme scenarios\n",
        "        extreme_scenarios = {\n",
        "            'market_crash': self._simulate_market_crash(base_data),\n",
        "            'rapid_growth': self._simulate_rapid_growth(base_data),\n",
        "            'high_volatility': self._simulate_high_volatility(base_data),\n",
        "            'stagnation': self._simulate_stagnation(base_data),\n",
        "        }\n",
        "        \n",
        "        extreme_results = {}\n",
        "        \n",
        "        for scenario_name, scenario_data in extreme_scenarios.items():\n",
        "            try:\n",
        "                prediction = self.model.predict_single_value(\n",
        "                    time_series=scenario_data,\n",
        "                    forecast_horizon=3\n",
        "                )\n",
        "                \n",
        "                # Analyze prediction characteristics\n",
        "                last_value = scenario_data[-1]\n",
        "                predicted_change = (prediction['mean_forecast'] - last_value) / last_value * 100\n",
        "                \n",
        "                extreme_results[scenario_name] = {\n",
        "                    'status': 'success',\n",
        "                    'prediction_value': prediction['mean_forecast'],\n",
        "                    'last_actual_value': last_value,\n",
        "                    'predicted_change_pct': predicted_change,\n",
        "                    'confidence_interval': prediction.get('confidence_interval')\n",
        "                }\n",
        "                \n",
        "            except Exception as e:\n",
        "                extreme_results[scenario_name] = {\n",
        "                    'status': 'failed',\n",
        "                    'error': str(e)\n",
        "                }\n",
        "        \n",
        "        self.robustness_results['extreme_scenarios'] = extreme_results\n",
        "        return extreme_results\n",
        "    \n",
        "    def _inject_missing_values(self, data: np.ndarray, missing_ratio: float) -> np.ndarray:\n",
        "        \"\"\"Inject missing values into data.\"\"\"\n",
        "        corrupted = data.copy()\n",
        "        n_missing = int(len(data) * missing_ratio)\n",
        "        missing_indices = np.random.choice(len(data), n_missing, replace=False)\n",
        "        corrupted[missing_indices] = np.nan\n",
        "        return corrupted\n",
        "    \n",
        "    def _inject_outliers(self, data: np.ndarray, outlier_ratio: float, magnitude: float) -> np.ndarray:\n",
        "        \"\"\"Inject outliers into data.\"\"\"\n",
        "        corrupted = data.copy()\n",
        "        n_outliers = int(len(data) * outlier_ratio)\n",
        "        outlier_indices = np.random.choice(len(data), n_outliers, replace=False)\n",
        "        \n",
        "        for idx in outlier_indices:\n",
        "            if np.random.random() > 0.5:\n",
        "                corrupted[idx] *= magnitude  # Extreme high\n",
        "            else:\n",
        "                corrupted[idx] /= magnitude  # Extreme low\n",
        "        \n",
        "        return corrupted\n",
        "    \n",
        "    def _add_gaussian_noise(self, data: np.ndarray, noise_ratio: float) -> np.ndarray:\n",
        "        \"\"\"Add Gaussian noise to data.\"\"\"\n",
        "        std = np.std(data) * noise_ratio\n",
        "        noise = np.random.normal(0, std, len(data))\n",
        "        return data + noise\n",
        "    \n",
        "    def _inject_trend_break(self, data: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Inject a trend break in the middle of the series.\"\"\"\n",
        "        corrupted = data.copy()\n",
        "        break_point = len(data) // 2\n",
        "        \n",
        "        # Add sudden level shift\n",
        "        level_shift = np.std(data) * 2\n",
        "        corrupted[break_point:] += level_shift\n",
        "        \n",
        "        return corrupted\n",
        "    \n",
        "    def _simulate_market_crash(self, data: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Simulate a market crash scenario.\"\"\"\n",
        "        crash_data = data.copy()\n",
        "        crash_start = int(len(data) * 0.8)  # Crash in last 20%\n",
        "        \n",
        "        for i in range(crash_start, len(data)):\n",
        "            crash_data[i] = crash_data[i-1] * 0.95  # 5% decline per period\n",
        "        \n",
        "        return crash_data\n",
        "    \n",
        "    def _simulate_rapid_growth(self, data: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Simulate rapid growth scenario.\"\"\"\n",
        "        growth_data = data.copy()\n",
        "        growth_start = int(len(data) * 0.8)\n",
        "        \n",
        "        for i in range(growth_start, len(data)):\n",
        "            growth_data[i] = growth_data[i-1] * 1.05  # 5% growth per period\n",
        "        \n",
        "        return growth_data\n",
        "    \n",
        "    def _simulate_high_volatility(self, data: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Simulate high volatility scenario.\"\"\"\n",
        "        volatile_data = data.copy()\n",
        "        volatility_start = int(len(data) * 0.7)\n",
        "        \n",
        "        for i in range(volatility_start, len(data)):\n",
        "            random_change = np.random.normal(0, 0.1)  # 10% std volatility\n",
        "            volatile_data[i] = volatile_data[i-1] * (1 + random_change)\n",
        "        \n",
        "        return volatile_data\n",
        "    \n",
        "    def _simulate_stagnation(self, data: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Simulate market stagnation scenario.\"\"\"\n",
        "        stagnant_data = data.copy()\n",
        "        stagnation_start = int(len(data) * 0.8)\n",
        "        \n",
        "        # Keep values roughly constant with small random variations\n",
        "        base_value = data[stagnation_start]\n",
        "        for i in range(stagnation_start, len(data)):\n",
        "            stagnant_data[i] = base_value * (1 + np.random.normal(0, 0.01))  # 1% std variation\n",
        "        \n",
        "        return stagnant_data\n",
        "    \n",
        "    def generate_robustness_report(self) -> Dict[str, Any]:\n",
        "        \"\"\"Generate comprehensive robustness report.\"\"\"\n",
        "        total_tests = 0\n",
        "        passed_tests = 0\n",
        "        \n",
        "        for test_category, results in self.robustness_results.items():\n",
        "            for test_result in results.values():\n",
        "                total_tests += 1\n",
        "                if test_result.get('status') == 'success':\n",
        "                    passed_tests += 1\n",
        "        \n",
        "        # Calculate stability scores\n",
        "        stability_scores = []\n",
        "        if 'data_corruption' in self.robustness_results:\n",
        "            for result in self.robustness_results['data_corruption'].values():\n",
        "                if 'stability_score' in result:\n",
        "                    stability_scores.append(result['stability_score'])\n",
        "        \n",
        "        return {\n",
        "            'overall_robustness': 'robust' if passed_tests / total_tests > 0.8 else 'needs_improvement',\n",
        "            'tests_passed': passed_tests,\n",
        "            'total_tests': total_tests,\n",
        "            'robustness_score': (passed_tests / total_tests * 100) if total_tests > 0 else 0,\n",
        "            'average_stability_score': np.mean(stability_scores) if stability_scores else None,\n",
        "            'detailed_results': self.robustness_results,\n",
        "            'timestamp': datetime.now().isoformat()\n",
        "        }\n",
        "\n",
        "# Run robustness testing\n",
        "if performance_summary.get('status') != 'skipped':\n",
        "    print(\"Running robustness testing...\\n\")\n",
        "    \n",
        "    robustness_tester = RobustnessTester(model, test_data)\n",
        "    \n",
        "    # Run all robustness tests\n",
        "    robustness_tester.test_data_corruption()\n",
        "    robustness_tester.test_input_variations()\n",
        "    robustness_tester.test_extreme_scenarios()\n",
        "    \n",
        "    # Generate robustness report\n",
        "    robustness_report = robustness_tester.generate_robustness_report()\n",
        "    \n",
        "    print(f\"\\nRobustness Testing Results:\")\n",
        "    print(f\"Overall Robustness: {robustness_report['overall_robustness'].upper()}\")\n",
        "    print(f\"Robustness Score: {robustness_report['robustness_score']:.1f}% ({robustness_report['tests_passed']}/{robustness_report['total_tests']})\")\n",
        "    \n",
        "    if robustness_report['average_stability_score']:\n",
        "        print(f\"Average Stability Score: {robustness_report['average_stability_score']:.1f}/100\")\n",
        "    \n",
        "    # Detailed results summary\n",
        "    for category, results in robustness_report['detailed_results'].items():\n",
        "        passed_in_category = sum(1 for r in results.values() if r.get('status') == 'success')\n",
        "        total_in_category = len(results)\n",
        "        print(f\"  {category.replace('_', ' ').title()}: {passed_in_category}/{total_in_category} passed\")\n",
        "    \n",
        "else:\n",
        "    print(\"WARNING: Skipping robustness testing\")\n",
        "    robustness_report = {'overall_robustness': 'skipped'}\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Results Visualization & Analysis\n",
        "\n",
        "Create comprehensive visualizations of test results and model performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create comprehensive visualizations\n",
        "def create_testing_visualizations():\n",
        "    \"\"\"Create comprehensive visualizations of testing results.\"\"\"\n",
        "    \n",
        "    # 1. Performance Metrics Dashboard\n",
        "    if performance_summary.get('status') != 'skipped' and 'overall_performance' in performance_summary:\n",
        "        fig = make_subplots(\n",
        "            rows=2, cols=2,\n",
        "            subplot_titles=('MAE Distribution', 'RÂ² Scores', 'MAPE Distribution', 'Directional Accuracy'),\n",
        "            specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
        "                   [{\"secondary_y\": False}, {\"secondary_y\": False}]]\n",
        "        )\n",
        "        \n",
        "        # Extract metrics from detailed results\n",
        "        all_metrics = []\n",
        "        zip_labels = []\n",
        "        for zip_code, zip_results in performance_summary['detailed_results']['accuracy_tests'].items():\n",
        "            for horizon, horizon_results in zip_results.items():\n",
        "                if 'mean_metrics' in horizon_results:\n",
        "                    metrics = horizon_results['mean_metrics'].copy()\n",
        "                    metrics['zip_code'] = zip_code\n",
        "                    metrics['horizon'] = horizon\n",
        "                    all_metrics.append(metrics)\n",
        "                    zip_labels.append(f\"{zip_code}-{horizon}\")\n",
        "        \n",
        "        if all_metrics:\n",
        "            metrics_df = pd.DataFrame(all_metrics)\n",
        "            \n",
        "            # MAE\n",
        "            fig.add_trace(\n",
        "                go.Histogram(x=metrics_df['mae'], name='MAE', nbinsx=10),\n",
        "                row=1, col=1\n",
        "            )\n",
        "            \n",
        "            # RÂ²\n",
        "            fig.add_trace(\n",
        "                go.Scatter(x=zip_labels, y=metrics_df['r2'], mode='markers+lines', name='RÂ²'),\n",
        "                row=1, col=2\n",
        "            )\n",
        "            \n",
        "            # MAPE\n",
        "            fig.add_trace(\n",
        "                go.Histogram(x=metrics_df['mape'], name='MAPE (%)', nbinsx=10),\n",
        "                row=2, col=1\n",
        "            )\n",
        "            \n",
        "            # Directional Accuracy\n",
        "            fig.add_trace(\n",
        "                go.Bar(x=zip_labels, y=metrics_df['directional_accuracy'], name='Dir. Acc. (%)'),\n",
        "                row=2, col=2\n",
        "            )\n",
        "            \n",
        "            fig.update_layout(\n",
        "                title_text=\"Model Performance Metrics Dashboard\",\n",
        "                height=600,\n",
        "                showlegend=False\n",
        "            )\n",
        "            fig.show()\n",
        "    \n",
        "    # 2. Response Time Analysis\n",
        "    if functionality_report.get('overall_status') != 'skipped':\n",
        "        response_times = functionality_report['detailed_results'].get('response_times', {})\n",
        "        if response_times:\n",
        "            sizes = []\n",
        "            times = []\n",
        "            for size_key, time_data in response_times.items():\n",
        "                if time_data.get('avg_response_time_ms'):\n",
        "                    size = int(size_key.split('_')[-1])\n",
        "                    sizes.append(size)\n",
        "                    times.append(time_data['avg_response_time_ms'])\n",
        "            \n",
        "            if sizes and times:\n",
        "                fig = go.Figure()\n",
        "                fig.add_trace(go.Scatter(\n",
        "                    x=sizes, y=times,\n",
        "                    mode='markers+lines',\n",
        "                    name='Response Time',\n",
        "                    line=dict(color='blue', width=2),\n",
        "                    marker=dict(size=8)\n",
        "                ))\n",
        "                \n",
        "                # Add performance threshold line\n",
        "                fig.add_hline(\n",
        "                    y=TEST_CONFIG['performance_thresholds']['max_response_time_ms'],\n",
        "                    line_dash=\"dash\",\n",
        "                    line_color=\"red\",\n",
        "                    annotation_text=\"Performance Threshold\"\n",
        "                )\n",
        "                \n",
        "                fig.update_layout(\n",
        "                    title=\"Response Time vs Input Data Size\",\n",
        "                    xaxis_title=\"Input Data Size (months)\",\n",
        "                    yaxis_title=\"Response Time (ms)\",\n",
        "                    height=400\n",
        "                )\n",
        "                fig.show()\n",
        "    \n",
        "    # 3. Robustness Test Results\n",
        "    if robustness_report.get('overall_robustness') != 'skipped':\n",
        "        categories = []\n",
        "        success_rates = []\n",
        "        \n",
        "        for category, results in robustness_report['detailed_results'].items():\n",
        "            passed = sum(1 for r in results.values() if r.get('status') == 'success')\n",
        "            total = len(results)\n",
        "            success_rate = (passed / total * 100) if total > 0 else 0\n",
        "            \n",
        "            categories.append(category.replace('_', ' ').title())\n",
        "            success_rates.append(success_rate)\n",
        "        \n",
        "        if categories and success_rates:\n",
        "            fig = go.Figure(data=[\n",
        "                go.Bar(x=categories, y=success_rates, text=[f\"{rate:.1f}%\" for rate in success_rates])\n",
        "            ])\n",
        "            \n",
        "            fig.add_hline(y=80, line_dash=\"dash\", line_color=\"orange\", \n",
        "                         annotation_text=\"Minimum Robustness Threshold (80%)\")\n",
        "            \n",
        "            fig.update_layout(\n",
        "                title=\"Robustness Test Results by Category\",\n",
        "                xaxis_title=\"Test Category\",\n",
        "                yaxis_title=\"Success Rate (%)\",\n",
        "                height=400\n",
        "            )\n",
        "            fig.update_traces(textposition='outside')\n",
        "            fig.show()\n",
        "    \n",
        "    # 4. Prediction Example Visualization\n",
        "    if test_data:\n",
        "        zip_code = list(test_data.keys())[0]\n",
        "        ts_data = test_data[zip_code]\n",
        "        \n",
        "        try:\n",
        "            # Make a prediction for visualization\n",
        "            prediction = model.predict(\n",
        "                time_series=ts_data.values,\n",
        "                forecast_horizon=12,\n",
        "                num_samples=100\n",
        "            )\n",
        "            \n",
        "            # Create forecast visualization\n",
        "            historical_dates = ts_data.index\n",
        "            forecast_dates = pd.date_range(\n",
        "                start=historical_dates[-1] + pd.DateOffset(months=1),\n",
        "                periods=12,\n",
        "                freq='MS'\n",
        "            )\n",
        "            \n",
        "            fig = go.Figure()\n",
        "            \n",
        "            # Historical data\n",
        "            fig.add_trace(go.Scatter(\n",
        "                x=historical_dates,\n",
        "                y=ts_data.values,\n",
        "                mode='lines',\n",
        "                name='Historical Prices',\n",
        "                line=dict(color='blue', width=2)\n",
        "            ))\n",
        "            \n",
        "            # Forecast mean\n",
        "            fig.add_trace(go.Scatter(\n",
        "                x=forecast_dates,\n",
        "                y=prediction['mean'],\n",
        "                mode='lines',\n",
        "                name='Forecast Mean',\n",
        "                line=dict(color='red', width=2)\n",
        "            ))\n",
        "            \n",
        "            # Confidence intervals\n",
        "            ci_90_upper = prediction['confidence_intervals']['p90']\n",
        "            ci_90_lower = prediction['confidence_intervals']['p10']\n",
        "            \n",
        "            fig.add_trace(go.Scatter(\n",
        "                x=list(forecast_dates) + list(forecast_dates[::-1]),\n",
        "                y=list(ci_90_upper) + list(ci_90_lower[::-1]),\n",
        "                fill='toself',\n",
        "                fillcolor='rgba(255,0,0,0.2)',\n",
        "                line=dict(color='rgba(255,255,255,0)'),\n",
        "                name='90% Confidence Interval',\n",
        "                showlegend=True\n",
        "            ))\n",
        "            \n",
        "            fig.update_layout(\n",
        "                title=f\"Home Price Forecast Example - ZIP {zip_code}\",\n",
        "                xaxis_title=\"Date\",\n",
        "                yaxis_title=\"Home Price ($)\",\n",
        "                height=500,\n",
        "                hovermode='x unified'\n",
        "            )\n",
        "            fig.show()\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"Could not create prediction visualization: {e}\")\n",
        "\n",
        "# Generate visualizations\n",
        "print(\"Creating testing result visualizations...\\n\")\n",
        "create_testing_visualizations()\n",
        "print(\"Visualizations completed\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Production Readiness Assessment\n",
        "\n",
        "Evaluate the model's readiness for production deployment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Production Readiness Assessment\n",
        "class ProductionReadinessAssessor:\n",
        "    def __init__(self, health_report, functionality_report, performance_summary, robustness_report):\n",
        "        self.health_report = health_report\n",
        "        self.functionality_report = functionality_report\n",
        "        self.performance_summary = performance_summary\n",
        "        self.robustness_report = robustness_report\n",
        "        self.assessment_criteria = {\n",
        "            'model_health': {'weight': 0.25, 'threshold': 100},\n",
        "            'functionality': {'weight': 0.25, 'threshold': 90},\n",
        "            'performance': {'weight': 0.30, 'threshold': 80},\n",
        "            'robustness': {'weight': 0.20, 'threshold': 80}\n",
        "        }\n",
        "    \n",
        "    def assess_model_health(self) -> Dict[str, Any]:\n",
        "        \"\"\"Assess model health score.\"\"\"\n",
        "        if self.health_report.get('overall_health') == 'healthy':\n",
        "            score = 100\n",
        "            status = 'excellent'\n",
        "        elif self.health_report.get('overall_health') == 'issues_detected':\n",
        "            passed_ratio = self.health_report.get('tests_passed', 0) / max(self.health_report.get('total_tests', 1), 1)\n",
        "            score = passed_ratio * 100\n",
        "            status = 'good' if score >= 80 else 'needs_improvement'\n",
        "        else:\n",
        "            score = 0\n",
        "            status = 'critical'\n",
        "        \n",
        "        return {\n",
        "            'score': score,\n",
        "            'status': status,\n",
        "            'details': self.health_report\n",
        "        }\n",
        "    \n",
        "    def assess_functionality(self) -> Dict[str, Any]:\n",
        "        \"\"\"Assess functionality score.\"\"\"\n",
        "        if self.functionality_report.get('overall_status') == 'passed':\n",
        "            score = 100\n",
        "            status = 'excellent'\n",
        "        elif self.functionality_report.get('overall_status') == 'partial':\n",
        "            score = self.functionality_report.get('success_rate', 0)\n",
        "            status = 'good' if score >= 90 else 'needs_improvement'\n",
        "        else:\n",
        "            score = 0\n",
        "            status = 'critical'\n",
        "        \n",
        "        return {\n",
        "            'score': score,\n",
        "            'status': status,\n",
        "            'details': self.functionality_report\n",
        "        }\n",
        "    \n",
        "    def assess_performance(self) -> Dict[str, Any]:\n",
        "        \"\"\"Assess performance score.\"\"\"\n",
        "        if self.performance_summary.get('status') == 'skipped':\n",
        "            return {'score': 0, 'status': 'not_tested', 'details': 'Performance testing was skipped'}\n",
        "        \n",
        "        score_components = []\n",
        "        \n",
        "        # Check overall performance metrics\n",
        "        if 'overall_performance' in self.performance_summary:\n",
        "            perf = self.performance_summary['overall_performance']\n",
        "            \n",
        "            # MAPE score (lower is better, target < 15%)\n",
        "            mape_score = max(0, 100 - (perf['mean_mape'] / 15 * 100))\n",
        "            score_components.append(mape_score)\n",
        "            \n",
        "            # RÂ² score (higher is better, target > 0.7)\n",
        "            r2_score = min(100, (perf['mean_r2'] / 0.7) * 100)\n",
        "            score_components.append(r2_score)\n",
        "            \n",
        "            # Directional accuracy (target > 60%)\n",
        "            dir_acc_score = min(100, (perf['mean_directional_accuracy'] / 60) * 100)\n",
        "            score_components.append(dir_acc_score)\n",
        "        \n",
        "        # Check baseline improvement\n",
        "        if 'baseline_performance' in self.performance_summary:\n",
        "            baseline = self.performance_summary['baseline_performance']\n",
        "            improvement_score = min(100, baseline['percent_improved'])\n",
        "            score_components.append(improvement_score)\n",
        "        \n",
        "        if score_components:\n",
        "            score = np.mean(score_components)\n",
        "            if score >= 90:\n",
        "                status = 'excellent'\n",
        "            elif score >= 80:\n",
        "                status = 'good'\n",
        "            elif score >= 60:\n",
        "                status = 'acceptable'\n",
        "            else:\n",
        "                status = 'needs_improvement'\n",
        "        else:\n",
        "            score = 0\n",
        "            status = 'not_assessed'\n",
        "        \n",
        "        return {\n",
        "            'score': score,\n",
        "            'status': status,\n",
        "            'details': self.performance_summary\n",
        "        }\n",
        "    \n",
        "    def assess_robustness(self) -> Dict[str, Any]:\n",
        "        \"\"\"Assess robustness score.\"\"\"\n",
        "        if self.robustness_report.get('overall_robustness') == 'skipped':\n",
        "            return {'score': 0, 'status': 'not_tested', 'details': 'Robustness testing was skipped'}\n",
        "        \n",
        "        score = self.robustness_report.get('robustness_score', 0)\n",
        "        \n",
        "        if score >= 90:\n",
        "            status = 'excellent'\n",
        "        elif score >= 80:\n",
        "            status = 'good'\n",
        "        elif score >= 60:\n",
        "            status = 'acceptable'\n",
        "        else:\n",
        "            status = 'needs_improvement'\n",
        "        \n",
        "        return {\n",
        "            'score': score,\n",
        "            'status': status,\n",
        "            'details': self.robustness_report\n",
        "        }\n",
        "    \n",
        "    def calculate_overall_readiness(self) -> Dict[str, Any]:\n",
        "        \"\"\"Calculate overall production readiness score.\"\"\"\n",
        "        assessments = {\n",
        "            'model_health': self.assess_model_health(),\n",
        "            'functionality': self.assess_functionality(),\n",
        "            'performance': self.assess_performance(),\n",
        "            'robustness': self.assess_robustness()\n",
        "        }\n",
        "        \n",
        "        # Calculate weighted score\n",
        "        total_score = 0\n",
        "        total_weight = 0\n",
        "        \n",
        "        for category, assessment in assessments.items():\n",
        "            if assessment['score'] > 0:  # Only include tested categories\n",
        "                weight = self.assessment_criteria[category]['weight']\n",
        "                total_score += assessment['score'] * weight\n",
        "                total_weight += weight\n",
        "        \n",
        "        overall_score = total_score / total_weight if total_weight > 0 else 0\n",
        "        \n",
        "        # Determine readiness level\n",
        "        if overall_score >= 90:\n",
        "            readiness_level = 'production_ready'\n",
        "            recommendation = 'Model is ready for production deployment'\n",
        "        elif overall_score >= 80:\n",
        "            readiness_level = 'mostly_ready'\n",
        "            recommendation = 'Model is mostly ready with minor improvements needed'\n",
        "        elif overall_score >= 70:\n",
        "            readiness_level = 'needs_improvement'\n",
        "            recommendation = 'Model needs significant improvements before production'\n",
        "        else:\n",
        "            readiness_level = 'not_ready'\n",
        "            recommendation = 'Model is not ready for production deployment'\n",
        "        \n",
        "        # Identify critical issues\n",
        "        critical_issues = []\n",
        "        for category, assessment in assessments.items():\n",
        "            threshold = self.assessment_criteria[category]['threshold']\n",
        "            if assessment['score'] < threshold:\n",
        "                critical_issues.append(f\"{category}: {assessment['status']} (score: {assessment['score']:.1f})\")\n",
        "        \n",
        "        return {\n",
        "            'overall_score': overall_score,\n",
        "            'readiness_level': readiness_level,\n",
        "            'recommendation': recommendation,\n",
        "            'critical_issues': critical_issues,\n",
        "            'category_assessments': assessments,\n",
        "            'assessment_timestamp': datetime.now().isoformat()\n",
        "        }\n",
        "\n",
        "# Run production readiness assessment\n",
        "print(\"Assessing production readiness...\\n\")\n",
        "\n",
        "assessor = ProductionReadinessAssessor(\n",
        "    health_report, functionality_report, performance_summary, robustness_report\n",
        ")\n",
        "\n",
        "readiness_assessment = assessor.calculate_overall_readiness()\n",
        "\n",
        "print(\"Production Readiness Assessment Results:\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"Overall Score: {readiness_assessment['overall_score']:.1f}/100\")\n",
        "print(f\"Readiness Level: {readiness_assessment['readiness_level'].replace('_', ' ').upper()}\")\n",
        "print(f\"Recommendation: {readiness_assessment['recommendation']}\")\n",
        "\n",
        "print(\"\\nCategory Breakdown:\")\n",
        "for category, assessment in readiness_assessment['category_assessments'].items():\n",
        "    status_indicator = {\n",
        "        'excellent': 'EXCELLENT',\n",
        "        'good': 'GOOD', \n",
        "        'acceptable': 'ACCEPTABLE',\n",
        "        'needs_improvement': 'NEEDS_IMPROVEMENT',\n",
        "        'critical': 'CRITICAL',\n",
        "        'not_tested': 'NOT_TESTED'\n",
        "    }.get(assessment['status'], 'UNKNOWN')\n",
        "    \n",
        "    print(f\"  {category.replace('_', ' ').title()}: {assessment['score']:.1f}/100 ({status_indicator})\")\n",
        "\n",
        "if readiness_assessment['critical_issues']:\n",
        "    print(\"\\nCritical Issues to Address:\")\n",
        "    for issue in readiness_assessment['critical_issues']:\n",
        "        print(f\"  - {issue}\")\n",
        "\n",
        "print(\"\\nProduction Deployment Checklist:\")\n",
        "checklist_items = [\n",
        "    (\"Model Health Check\", readiness_assessment['category_assessments']['model_health']['score'] >= 90),\n",
        "    (\"Functionality Tests\", readiness_assessment['category_assessments']['functionality']['score'] >= 90),\n",
        "    (\"Performance Benchmarks\", readiness_assessment['category_assessments']['performance']['score'] >= 80),\n",
        "    (\"Robustness Testing\", readiness_assessment['category_assessments']['robustness']['score'] >= 80),\n",
        "    (\"Response Time < 5s\", True),  # Assume passed if functionality tests passed\n",
        "    (\"Error Handling\", True),      # Assume passed if robustness tests passed\n",
        "    (\"Documentation Complete\", True),  # This notebook serves as documentation\n",
        "    (\"Monitoring Setup\", False),   # Would need to be implemented separately\n",
        "]\n",
        "\n",
        "for item, passed in checklist_items:\n",
        "    status = \"PASS\" if passed else \"FAIL\"\n",
        "    print(f\"  {status}: {item}\")\n",
        "\n",
        "# Save assessment results\n",
        "assessment_output = {\n",
        "    'model_testing_summary': {\n",
        "        'health_report': health_report,\n",
        "        'functionality_report': functionality_report,\n",
        "        'performance_summary': performance_summary,\n",
        "        'robustness_report': robustness_report,\n",
        "        'production_readiness': readiness_assessment\n",
        "    },\n",
        "    'test_configuration': TEST_CONFIG,\n",
        "    'statistical_configuration': STAT_CONFIG,\n",
        "    'timestamp': datetime.now().isoformat()\n",
        "}\n",
        "\n",
        "# Save to JSON file\n",
        "output_path = Path('../outputs/model_testing_results.json')\n",
        "output_path.parent.mkdir(exist_ok=True)\n",
        "\n",
        "with open(output_path, 'w') as f:\n",
        "    json.dump(assessment_output, f, indent=2, default=str)\n",
        "\n",
        "print(f\"\\nTest results saved to: {output_path}\")\n",
        "print(\"Production readiness assessment completed\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Conclusions & Recommendations\n",
        "\n",
        "Summary of testing results and recommendations for model deployment and improvements.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate comprehensive conclusions and recommendations\n",
        "def generate_conclusions_and_recommendations():\n",
        "    \"\"\"Generate comprehensive conclusions and recommendations based on test results.\"\"\"\n",
        "    \n",
        "    print(\"COMPREHENSIVE MODEL TESTING CONCLUSIONS\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    # Executive Summary\n",
        "    print(\"\\nEXECUTIVE SUMMARY\")\n",
        "    print(\"-\" * 30)\n",
        "    \n",
        "    overall_score = readiness_assessment.get('overall_score', 0)\n",
        "    readiness_level = readiness_assessment.get('readiness_level', 'unknown')\n",
        "    \n",
        "    print(f\"â¢ Overall Model Score: {overall_score:.1f}/100\")\n",
        "    print(f\"â¢ Production Readiness: {readiness_level.replace('_', ' ').title()}\")\n",
        "    print(f\"â¢ Primary Recommendation: {readiness_assessment.get('recommendation', 'Assessment incomplete')}\")\n",
        "    \n",
        "    # Detailed Findings\n",
        "    print(\"\\nDETAILED FINDINGS\")\n",
        "    print(\"-\" * 30)\n",
        "    \n",
        "    # Model Health\n",
        "    health_score = readiness_assessment['category_assessments']['model_health']['score']\n",
        "    print(f\"â¢ Model Health: {health_score:.1f}/100\")\n",
        "    if health_score >= 90:\n",
        "        print(\"  PASS: Model loads correctly and passes all health checks\")\n",
        "    else:\n",
        "        print(\"  WARNING: Model health issues detected - review initialization and dependencies\")\n",
        "    \n",
        "    # Functionality\n",
        "    func_score = readiness_assessment['category_assessments']['functionality']['score']\n",
        "    print(f\"â¢ Functionality: {func_score:.1f}/100\")\n",
        "    if func_score >= 90:\n",
        "        print(\"  PASS: All core functionality working as expected\")\n",
        "    else:\n",
        "        print(\"  WARNING: Some functionality issues detected - review error handling\")\n",
        "    \n",
        "    # Performance\n",
        "    perf_score = readiness_assessment['category_assessments']['performance']['score']\n",
        "    print(f\"â¢ Performance: {perf_score:.1f}/100\")\n",
        "    if perf_score >= 80:\n",
        "        print(\"  PASS: Performance meets production requirements\")\n",
        "        if 'overall_performance' in performance_summary:\n",
        "            perf = performance_summary['overall_performance']\n",
        "            print(f\"    - Mean MAPE: {perf['mean_mape']:.1f}%\")\n",
        "            print(f\"    - Mean RÂ²: {perf['mean_r2']:.3f}\")\n",
        "            print(f\"    - Directional Accuracy: {perf['mean_directional_accuracy']:.1f}%\")\n",
        "    else:\n",
        "        print(\"  WARNING: Performance below production standards\")\n",
        "    \n",
        "    # Robustness\n",
        "    robust_score = readiness_assessment['category_assessments']['robustness']['score']\n",
        "    print(f\"â¢ Robustness: {robust_score:.1f}/100\")\n",
        "    if robust_score >= 80:\n",
        "        print(\"  PASS: Model demonstrates good robustness to data issues\")\n",
        "    else:\n",
        "        print(\"  WARNING: Model shows sensitivity to data quality issues\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"TESTING COMPLETED SUCCESSFULLY\")\n",
        "    print(f\"Assessment Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "    print(f\"Results saved to: outputs/model_testing_results.json\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "# Generate final conclusions and recommendations\n",
        "generate_conclusions_and_recommendations()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
